{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying RRL stars in the PAL5 globular cluster\n",
    "\n",
    "This notebook aims to find any RR Lyrae stars within the Palomar 5 globular cluster and construct light curves of them. This program starts by performing PSF photometry on the 12 time epochs, complete with uncertainties and corrections, for both the 3.6um and 4.5um channels. Once the photometry has been completed, each epoch's stars will be matched with the either the previous epoch or epoch 01 using dummy variables and saved to a new file. These files will then be read in and the magnitudes for each relevant star(s) extracted and plotted against the time epochs to produce the lightcurves.\n",
    "\n",
    "Attempts will be made to introduce the functions and automation but may require significant manual editing of parameters for each run through.\n",
    "\n",
    "Contents: 1) list of imports\n",
    "\n",
    "2) PSF Photometry: -read in epoch01 and build ePSF model -perform PSF photometry using ePSF model for all epochs within a glob.glob loop, saving the files in the format: PAL5_PSFphot_01_epochxx_channelxpxum.fits\n",
    "\n",
    "3) Star Matching: using WCS match stars in each epoch with either the epoch before or epoch 01 and save to file to avoid duplicating the code 6+ times (use dummy variables and iterate over file number?), saved as: PAL5_01_epochxx_channel_xpxum_matched.fits\n",
    "\n",
    "4) Extracting magnitudes: reload in the files and extract the magnitudes and uncertainties for stars of interest using the catalog at: http://www.astro.utoronto.ca/~cclement/cat/C1513p000\n",
    "\n",
    "5) Lightcurves: plot the lightcurves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from astropy import wcs\n",
    "from astropy import units as u\n",
    "from astropy.io import fits # used for FITS file management\n",
    "from astropy.io import ascii\n",
    "from astropy.time import Time\n",
    "from astropy.stats import sigma_clipped_stats # used within star detection\n",
    "from astropy.table import Table\n",
    "from astropy.nddata import NDData\n",
    "from astropy.coordinates import SkyCoord\n",
    "from astropy.coordinates import match_coordinates_sky\n",
    "from astropy.coordinates import Angle\n",
    "from astropy.visualization import simple_norm\n",
    "from astropy.modeling.fitting import LevMarLSQFitter\n",
    "from photutils import aperture_photometry # used to perform photometry using annuli\n",
    "from photutils import DAOStarFinder # used for the star finding algorithm\n",
    "from photutils import CircularAperture, CircularAnnulus\n",
    "from photutils import EPSFBuilder\n",
    "from photutils.psf import extract_stars\n",
    "from photutils.psf import DAOGroup\n",
    "from photutils.psf import IterativelySubtractedPSFPhotometry\n",
    "from photutils.background import MMMBackground\n",
    "from matplotlib.colors import LogNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## BASE DATA DIRECTORY ##\n",
    "\n",
    "base_dir = 'PAL5_data/*/'\n",
    "general_dir = 'PAL5_data/'\n",
    "channel = '3p6um'\n",
    "\n",
    "## CHANNEL ##\n",
    "\n",
    "if channel == '3p6um':\n",
    "    aper_corr = 1.1233         # aperture correction for 337 (6,6,14) apertures in channel 1, given in IRAC handbook §4.10\n",
    "    ap_err = aper_corr * 0.02  # uncertainty to ~2% as per IRAC handbook\n",
    "    zmag = 18.80               # zeropoint magnitude given in IRAC handbook §4.8\n",
    "    zmag_err = 0.02            # uncertainty calculated from F0 = 280.9 +/- 4.1 in IRAC handbook\n",
    "elif channel == '4p5um':\n",
    "    aper_corr = 1.1336\n",
    "    ap_err = aper_corr * 0.02\n",
    "    zmag = 18.32\n",
    "    zmag_err = 0.02 \n",
    "    \n",
    "#____________________________________#\n",
    "## BUILD ePSF MODEL USING ONE IMAGE ##\n",
    "\n",
    "epoch_dir = 'PAL5_data/PAL5__e1/'\n",
    "\n",
    "epsf_file = epoch_dir+'PAL5__e1_'+channel+'.fits'\n",
    "\n",
    "## OPENING FITS FILE AND EXTRACTING DATA ##\n",
    "\n",
    "with fits.open(epsf_file) as header_list:\n",
    "    header = header_list[0].header\n",
    "    fluxconv = header['FLUXCONV']\n",
    "    exptime = header['EXPTIME']\n",
    "    time = Time(header['DATE_OBS'])\n",
    "    counts = exptime / fluxconv\n",
    "    image_data = fits.getdata(epsf_file, ext = 0)\n",
    "    data = image_data * counts\n",
    "    print('FITS file information:\\nFILE = {0}\\nDATE = {1}\\nFLUXCONV = {2}\\nEXPTIME = {3}\\n\\n'.format(epsf_file, time, fluxconv, exptime))\n",
    "\n",
    "## PARAMETERS : ePSF ##\n",
    "\n",
    "fwhm = 5.\n",
    "sigma_val = 6.\n",
    "model_threshold = 100.\n",
    "roundlo = -0.5\n",
    "roundhi = 0.5\n",
    "sharphi = 0.9\n",
    "do_plot = True\n",
    "\n",
    "mean, median, std = sigma_clipped_stats(data, sigma = sigma_val) # sigma-clipping on data\n",
    "\n",
    "starfind_init = DAOStarFinder(fwhm = fwhm, threshold = model_threshold * std, roundlo = roundlo, roundhi = roundhi, sharphi = sharphi)\n",
    "epsf_sources = starfind_init(data)\n",
    "print('Number of ePSF sources found = {0}\\n'.format(len(epsf_sources)))\n",
    "\n",
    "if do_plot == True:\n",
    "    # PLOT SELECTED STARS TO TEST PARAMETERS #\n",
    "    positions = np.transpose((epsf_sources['xcentroid'], epsf_sources['ycentroid']))\n",
    "    apertures = CircularAperture(positions, r = 6.)\n",
    "\n",
    "    plt.imshow(data, cmap = 'viridis', origin = 'lower', norm = LogNorm(), interpolation = 'nearest')\n",
    "    apertures.plot(color = 'black', lw = 1.)\n",
    "    plt.colorbar(fraction = 0.05)\n",
    "    plt.title('Selected ePSF model stars with params: threshold {0} * std, fwhm = {1}, round = ±{2}, sharphi = {3}'\n",
    "              .format(model_threshold, fwhm, roundhi, sharphi))\n",
    "    plt.grid(b = True, which = 'major', lw = .5, color = 'black')\n",
    "    plt.grid(b = True, which = 'minor', lw = .5, color = 'black')\n",
    "    plt.gcf().set_size_inches(15, 8)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "elif do_plot == False:\n",
    "    pass\n",
    "\n",
    "## STAR CUTOUTS FOR ePSF ##\n",
    "\n",
    "cutout_size = 200\n",
    "hsize = (cutout_size - 1) / 2\n",
    "x = epsf_sources['xcentroid']\n",
    "y = epsf_sources['ycentroid']\n",
    "mask = ((x > hsize) & (x < (data.shape[1] - 1 - hsize)) &\n",
    "       (y > hsize) & (y < (data.shape[0] - 1 - hsize)))\n",
    "\n",
    "star_tbl = Table()      # build table of star sources\n",
    "star_tbl['x'] = x[mask]\n",
    "star_tbl['y'] = y[mask]\n",
    "star_tbl['id'] = range(len(star_tbl))\n",
    "print('Number of refined ePSF sources = {0}\\n'.format(len(star_tbl)))\n",
    "\n",
    "if do_plot == True:\n",
    "    # PLOT CUTOUT STARS TO BE USED IN EPSF MODEL #\n",
    "    cutout_pos = np.transpose((star_tbl['x'], star_tbl['y']))\n",
    "    cutout_apers = CircularAperture(cutout_pos, r = 6.)\n",
    "\n",
    "    plt.imshow(data, cmap = 'viridis', origin = 'lower', norm = LogNorm(), interpolation = 'nearest')\n",
    "    cutout_apers.plot(color = 'black', lw = 1.)\n",
    "    plt.colorbar(fraction = 0.05)\n",
    "    plt.title('Selected cutout ePSF model stars with params: threshold {0} * std, fwhm = {1}, round = ±{2}, sharphi = {3}'\n",
    "              .format(model_threshold, fwhm, roundhi, sharphi))\n",
    "    plt.grid(b = True, which = 'major', lw = .5, color = 'black')\n",
    "    plt.grid(b = True, which = 'minor', lw = .5, color = 'black')\n",
    "    plt.gcf().set_size_inches(15, 8)\n",
    "    plt.show()\n",
    "    \n",
    "elif do_plot == False:\n",
    "    pass\n",
    "\n",
    "## EXTRACT STARS ##\n",
    "\n",
    "mean_val, median_val, std_val = sigma_clipped_stats(data, sigma = sigma_val)\n",
    "epsf_data = data - median_val\n",
    "\n",
    "while True:\n",
    "    nddata = NDData(data = epsf_data)\n",
    "    epsf_stars = extract_stars(nddata, star_tbl, size = 25)\n",
    "\n",
    "    ## VISUALISE MODEL STARS ##\n",
    "    ncols = 6\n",
    "    nrows = int(np.floor(len(epsf_stars) / ncols))\n",
    "    remaining = len(epsf_stars) - ncols * nrows\n",
    "\n",
    "    fig, ax = plt.subplots(nrows = nrows, ncols = ncols, figsize = (20,20), squeeze = True)\n",
    "    ax = ax.ravel()\n",
    "    for i in range(nrows * ncols):\n",
    "        norm = simple_norm(epsf_stars[i], 'log', percent = 99.)\n",
    "        ax[i].imshow(epsf_stars[i], cmap = 'viridis', norm = norm, origin = 'lower')\n",
    "        ax[i].set_title([i])\n",
    "    plt.show()\n",
    "    for i in range(ncols * nrows, len(epsf_stars)):\n",
    "        norm = simple_norm(epsf_stars[i], 'log', percent = 99.)\n",
    "        ax[i - ncols * nrows].imshow(epsf_stars[i], cmap = 'viridis', norm = norm, origin = 'lower')\n",
    "        ax[i - ncols * nrows].set_title([i])\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    ## BUILD ePSF ##\n",
    "\n",
    "    epsf_builder = EPSFBuilder(oversampling = 2, maxiters = 10, progress_bar = True)\n",
    "    epsf, fitter = epsf_builder(epsf_stars)\n",
    "\n",
    "    norm = simple_norm(epsf.data, 'log', percent = 99.)\n",
    "    plt.imshow(epsf.data, cmap = 'viridis', norm = norm, origin = 'lower')\n",
    "    plt.title('constructed ePSF model')\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "    \n",
    "    print('Remove any poor quality model stars from the model by their ID. Use ID -99 to escape')\n",
    "    remove = [int(k) for k in input().split()]\n",
    "    \n",
    "    if remove == [-99]:\n",
    "        print('Escape completed\\n')\n",
    "        break\n",
    "    \n",
    "    star_tbl.add_index('id')\n",
    "    bad_id = star_tbl.loc_indices[remove]\n",
    "    star_tbl.remove_rows(bad_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PSF PHOTOMETRY ON ALL EPOCHS ##\n",
    "\n",
    "## PARAMETERS ##\n",
    "\n",
    "sigma_psf = 4.\n",
    "fwhm = 5.\n",
    "r_ap = 6.\n",
    "r_in = 6.\n",
    "r_out = 14.\n",
    "roundlo = -0.5\n",
    "roundhi = 0.5\n",
    "sharphi = 1.0\n",
    "do_plot = False\n",
    "\n",
    "## EPOCH LOOP COUNTER ##\n",
    "\n",
    "epoch = 0\n",
    "\n",
    "## PSF PHOTOMETRY LOOP ##\n",
    "for file in glob.glob(base_dir+'PAL5__e[0-9]_'+channel+'.fits', recursive = True) + glob.glob(base_dir+'PAL5__e[0-9][0-9]_'+channel+'.fits', recursive = True):\n",
    "    epoch += 1\n",
    "    print('EPOCH NUMBER = {0}\\n'.format(epoch))\n",
    "    ## OPENING FITS FILE AND EXTRACTING DATA ##\n",
    "    with fits.open(file) as header_list:\n",
    "        header = header_list[0].header\n",
    "        fluxconv = header['FLUXCONV']\n",
    "        exptime = header['EXPTIME']\n",
    "        time = Time(header['DATE_OBS'])\n",
    "        counts = exptime / fluxconv\n",
    "        \n",
    "        image_data = fits.getdata(file, ext = 0)\n",
    "        data = image_data * counts\n",
    "        \n",
    "        print('FITS file information:\\nFILE = {0}\\nDATE = {1}\\nFLUXCONV = {2}\\nEXPTIME = {3}\\n\\n'.format(file, time, fluxconv, exptime))\n",
    "\n",
    "    ## EXTRACTING LOC-DEPENDENT CORRECTIONS ##\n",
    "    corr_file = general_dir+'PAL5__e'+str(epoch)+'/PAL5__e'+str(epoch)+'_correction_'+channel+'.fits'\n",
    "    with fits.open(corr_file) as hdu_list:\n",
    "        corr_data = hdu_list[0].data\n",
    "    \n",
    "    ## SOURCE DETECTION ON IMAGE ## \n",
    "    psf_daofind = DAOStarFinder(fwhm = fwhm, threshold = sigma_psf * std, roundlo = roundlo, roundhi = roundhi)\n",
    "    psf_sources = psf_daofind(data)\n",
    "\n",
    "    psf_positions = np.transpose((psf_sources['xcentroid'], psf_sources['ycentroid']))\n",
    "    psf_apertures = CircularAperture(psf_positions, r = 6.)\n",
    "\n",
    "    plt.imshow(data, cmap = 'viridis', origin = 'lower', norm = LogNorm(), interpolation = 'nearest')\n",
    "    psf_apertures.plot(color = 'black', lw = 1.)\n",
    "    plt.colorbar(fraction = 0.05)\n",
    "    plt.title('Detected PSF stars: threshold {0} * std, fwhm = {1}, round = ±{2}, sharphi = {3}'\n",
    "              .format(sigma_psf, fwhm, roundhi, sharphi))\n",
    "    plt.grid(b = True, which = 'major', lw = .5, color = 'black')\n",
    "    plt.grid(b = True, which = 'minor', lw = .5, color = 'black')\n",
    "    plt.gcf().set_size_inches(15, 8)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    print('Number of stars detected = {0}\\n'.format(len(psf_sources)))\n",
    "    \n",
    "    ## GROUP ##\n",
    "\n",
    "    psf_sources['xcentroid'].name = 'x_0'\n",
    "    psf_sources['ycentroid'].name = 'y_0'\n",
    "\n",
    "    daogroup = DAOGroup(crit_separation = sigma_psf * fwhm)\n",
    "    bkg_estimator = MMMBackground()\n",
    "    fitter = LevMarLSQFitter()\n",
    "\n",
    "    data_psf = np.nan_to_num(data, nan = 1**-7)\n",
    "\n",
    "    ## FIXED CENTROIDS ##\n",
    "\n",
    "    epsf.x_0.fixed = True\n",
    "    epsf.y_0.fixed = True\n",
    "    pos = Table(names = ['x_0', 'y_0'], data = [psf_sources['x_0'], psf_sources['y_0']])\n",
    "    \n",
    "    ## PERFORMING PSF PHOTOMETRY ##\n",
    "\n",
    "    PSF_photometry = IterativelySubtractedPSFPhotometry(finder = psf_daofind,\n",
    "                                                        group_maker = daogroup,\n",
    "                                                        bkg_estimator = bkg_estimator,\n",
    "                                                        psf_model = epsf,\n",
    "                                                        fitter = fitter,\n",
    "                                                        niters = 3,\n",
    "                                                        aperture_radius = 6.,\n",
    "                                                        fitshape = (11, 11))\n",
    "\n",
    "    result_phot = PSF_photometry(image = data_psf, init_guesses = pos)\n",
    "    residual_image = PSF_photometry.get_residual_image()\n",
    "    \n",
    "    #hdu = fits.PrimaryHDU(residual_image)\n",
    "    #hdul = fits.HDUList([hdu])\n",
    "    #hdul.writeto('residual_image_08_fixed_centroids.fits')\n",
    "    print('Number of PSF stars found and analysed = {0}\\n'.format(len(result_phot)))\n",
    "    \n",
    "    if do_plot == True:\n",
    "        ## VISUALISE PSF IMAGE AND RESIDUALS ##\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(data_psf, cmap = 'viridis', norm = LogNorm(), interpolation = 'nearest', origin = 'lower', vmin = 0.000001, vmax = 10**6)\n",
    "        plt.title('input data')\n",
    "        plt.colorbar(orientation = 'horizontal')\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(residual_image, cmap = 'viridis', norm = LogNorm(), interpolation = 'nearest', origin = 'lower', vmin = 0.000001, vmax = 10**6)\n",
    "        plt.title('residual image')\n",
    "        plt.colorbar(orientation = 'horizontal')\n",
    "        plt.gcf().set_size_inches(20, 14)\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "    elif do_plot == False:\n",
    "        pass\n",
    "    \n",
    "    phot = result_phot   # REDEFINE PHOTOMETRY TABLE FOR EASE\n",
    "    \n",
    "    ## PHOTOMETRY: UNCERTAINTIES ##\n",
    "    \n",
    "    PSF_err = phot['flux_unc']\n",
    "    PSF_flux = phot['flux_fit'] \n",
    "    \n",
    "    ## APPARENT MAGNITUDES ##\n",
    "    \n",
    "    phot['apparent_mag'] = float('NaN')\n",
    "    for i in range(0, len(phot)):\n",
    "        # APPLY ARRAY-LOC DEP CORRECTION\n",
    "        loc_corr = corr_data[int(phot['y_fit'][i])][int(phot['x_fit'][i])]\n",
    "        if phot['flux_fit'][i] >= 0:\n",
    "            phot['apparent_mag'][i] = zmag - 2.5 * math.log10(phot['flux_fit'][i] * aper_corr * loc_corr / counts)\n",
    "    \n",
    "    ## APPARENT MAGNITUDE: UNCERTAINTIES ##\n",
    "    \n",
    "    phot['apparent_mag_err'] = float('Nan')\n",
    "    for i in range(0, len(phot)):\n",
    "        if phot['flux_fit'][i] >= 0:\n",
    "            phot['apparent_mag_err'][i] = pow(zmag_err**2 + (2.5*(pow((PSF_err[i] / PSF_flux[i])**2 + (ap_err / aper_corr)**2, 0.5) / (np.log(10)))**2), 0.5)\n",
    "    \n",
    "    ## EXPORT PHOTOMETRY FILE AND PRINT TO SCREEN ##\n",
    "    phot['id', 'x_0', 'y_0', 'apparent_mag', 'apparent_mag_err'].write(\n",
    "        r'C:\\Users\\lukeb\\Documents\\MPhys_Project_RRLs\\Luke_RRLs_project\\output_files\\PAL5_PSFphot_01_epoch{0}_channel{1}.txt'.format(epoch, channel), format = 'csv', overwrite = False)\n",
    "    \n",
    "    print(phot['id', 'x_0', 'y_0', 'apparent_mag', 'apparent_mag_err'])\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MATCH STARS BETWEEN EPOCHS ##\n",
    "\n",
    "file_1 = r'C:/Users/lukeb/Documents/MPhys_Project_RRLs/Luke_RRLs_project/output_files/PAL5_PSFphot_01_epoch1_channel3p6um.txt'\n",
    "\n",
    "## EPOCH LOOP COUNTER ##\n",
    "\n",
    "epoch = 1\n",
    "\n",
    "time_list = [0]\n",
    "\n",
    "for file_2 in glob.glob(r'C:/Users/lukeb/Documents/MPhys_Project_RRLs/Luke_RRLs_project/output_files/PAL5_PSFphot_01_epoch[2-9]_channel'+channel+'.txt', recursive = True) + glob.glob(r'C:/Users/lukeb/Documents/MPhys_Project_RRLs/Luke_RRLs_project/output_files/PAL5_PSFphot_01_epoch[0-9][0-9]_channel'+channel+'.txt', recursive = True):\n",
    "    \n",
    "    epoch += 1\n",
    "    print('EPOCH NUMBER = {0}\\n'.format(epoch))\n",
    "    \n",
    "    image_1 = 'PAL5_data/PAL5__e1/PAL5__e1_3p6um.fits'\n",
    "    image_2 = str('PAL5_data/PAL5__e{0}/PAL5__e{0}_3p6um.fits'.format(epoch))\n",
    "    \n",
    "    with fits.open(image_1) as hdr_1_list:\n",
    "        hdr_1 = hdr_1_list[0].header\n",
    "        time1 = Time(hdr_1['DATE_OBS'])\n",
    "        \n",
    "    with fits.open(image_2) as hdr_2_list:\n",
    "        hdr_2 = hdr_2_list[0].header\n",
    "        time2 = Time(hdr_2['DATE_OBS'])\n",
    "    \n",
    "    delta_time = time2 - time1\n",
    "    time_list.append(delta_time.sec)\n",
    "    \n",
    "    f1 = ascii.read(file_1, delimiter = ',')\n",
    "    f2 = ascii.read(file_2, delimiter = ',')\n",
    "    \n",
    "    x_1 = f1['x_0']\n",
    "    y_1 = f1['y_0']\n",
    "    x_2 = f2['x_0']\n",
    "    y_2 = f2['y_0']\n",
    "    \n",
    "    w1 = wcs.WCS(hdr_1)\n",
    "    w2 = wcs.WCS(hdr_2)\n",
    "\n",
    "    coord_1 = np.transpose((x_1, y_1))\n",
    "    coord_2 = np.transpose((x_2, y_2))\n",
    "    world_1 = w1.wcs_pix2world(coord_1, 0)\n",
    "    world_2 = w2.wcs_pix2world(coord_2, 0)\n",
    "\n",
    "    ra_1, dec_1 = world_1[:, 0], world_1[:, 1]\n",
    "    ra_2, dec_2 = world_2[:, 0], world_2[:, 1]\n",
    "\n",
    "    c_1 = SkyCoord(ra_1, dec_1, frame = 'icrs', unit = 'deg')\n",
    "    c_2 = SkyCoord(ra_2, dec_2, frame = 'icrs', unit = 'deg')\n",
    "\n",
    "    idx, d2d, d3d = c_2.match_to_catalog_sky(c_1)\n",
    "\n",
    "    # APPEND RIGHT ASCENSION AND DECLINATION TO EXISTING FILES\n",
    "    f1['ra'] = ra_1\n",
    "    f1['dec'] = dec_1\n",
    "    f2['ra'] = ra_2\n",
    "    f2['dec'] = dec_2\n",
    "    \n",
    "    f1['ra_hms'] = str('null')\n",
    "    f1['dec_dms'] = str('null')\n",
    "    f2['ra_hms'] = str('null')\n",
    "    f2['dec_dms'] = str('null')\n",
    "    \n",
    "    # FOLLOWING TUTORIAL, ENSURE MATCHES ARE SIGNIFICANT\n",
    "    radius = 0.0001\n",
    "    selection = (d2d > radius*u.deg)\n",
    "    match_index = idx\n",
    "    match_index[selection] = -99.\n",
    "    matches = (match_index >= 0)\n",
    "\n",
    "    mag_1 = f1['apparent_mag'][match_index][matches]\n",
    "    mag_2 = f2['apparent_mag'][matches]\n",
    "    mag_1_err = f1['apparent_mag_err'][match_index][matches]\n",
    "    mag_2_err = f2['apparent_mag_err'][matches]\n",
    "    delta_mag = mag_1 - mag_2\n",
    "    print('Number of matched stars between epoch 1 and {0} = {1}\\n'.format(epoch, len(delta_mag)))\n",
    "    \n",
    "    # CONSTRUCT TABLE OF MATCHED STARS\n",
    "    matched_tbl_1 = Table()\n",
    "    matched_tbl_2 = Table()\n",
    "    ra_1m, dec_1m = f1['ra'][match_index][matches], f1['dec'][match_index][matches]\n",
    "    ra_2m, dec_2m = f2['ra'][matches], f2['dec'][matches]\n",
    "    matched_tbl_1['ra1'], matched_tbl_1['dec1'] = ra_1m, dec_1m\n",
    "    matched_tbl_2['ra2'], matched_tbl_2['dec2'] = ra_2m, dec_2m\n",
    "    matched_tbl_1['mag_1'] = mag_1\n",
    "    matched_tbl_2['mag_2'] = mag_2\n",
    "    matched_tbl_1['mag_1_err'] = mag_1_err\n",
    "    matched_tbl_2['mag_2_err'] = mag_2_err\n",
    "    \n",
    "    # INITIALIZE COLUMNS FOR CONVERTING RIGHT ASCENSION AND DECLINATION FOR COMPARISON TO STAR CATALOGUE\n",
    "    # RIGHT ASCENSION TO BE CONVERTED INTO HOURS:MINUTES:SECONDS\n",
    "    # DECLINATION TO BE CONVERTED INTO DEGRESS:MINUTES:SECONDS\n",
    "    matched_tbl_1['ra_hms'] = str('null')\n",
    "    matched_tbl_1['dec_dms'] = str('null')\n",
    "    matched_tbl_2['ra_hms'] = str('null')\n",
    "    matched_tbl_2['dec_dms'] = str('null')\n",
    "    \n",
    "    # CONVERTING STRING COLUMNS IN TABLES INTO OBJECT\n",
    "    for col in matched_tbl_1.itercols():\n",
    "        if col.dtype.kind in 'SU':\n",
    "            matched_tbl_1.replace_column(col.name, col.astype('object'))\n",
    "    for col in matched_tbl_2.itercols():\n",
    "        if col.dtype.kind in 'SU':\n",
    "            matched_tbl_2.replace_column(col.name, col.astype('object'))\n",
    "    \n",
    "    # PERFORMING CONVERSIONS\n",
    "    for i in range(len(matched_tbl_1)):\n",
    "        ra_deg = Angle(matched_tbl_1['ra1'][i], u.deg)\n",
    "        matched_tbl_1['ra_hms'][i] = ra_deg.to_string(unit = u.hour, sep=':')\n",
    "    for i in range(len(matched_tbl_1)):\n",
    "        dec_deg = Angle(matched_tbl_1['dec1'][i], u.deg)\n",
    "        matched_tbl_1['dec_dms'][i] = dec_deg.to_string(unit = u.degree, sep=':')\n",
    "    for i in range(len(matched_tbl_2)):\n",
    "        ra_deg = Angle(matched_tbl_2['ra2'][i], u.deg)\n",
    "        matched_tbl_2['ra_hms'][i] = ra_deg.to_string(unit = u.hour, sep=':')\n",
    "    for i in range(len(matched_tbl_2)):\n",
    "        dec_deg = Angle(matched_tbl_2['dec2'][i], u.deg)\n",
    "        matched_tbl_2['dec_dms'][i] = dec_deg.to_string(unit = u.degree, sep=':')\n",
    "    \n",
    "    # EXPORT CONVERTED TABLES\n",
    "    matched_tbl_1['ra_hms', 'dec_dms', 'mag_1', 'mag_1_err'].write(r'C:\\Users\\lukeb\\Documents\\MPhys_Project_RRLs\\Luke_RRLs_project\\output_files\\PAL5_matched_stars_01_epoch1_{0}.txt'.format(channel), format = 'csv', overwrite = True)\n",
    "    matched_tbl_2['ra_hms', 'dec_dms', 'mag_2', 'mag_2_err'].write(r'C:\\Users\\lukeb\\Documents\\MPhys_Project_RRLs\\Luke_RRLs_project\\output_files\\PAL5_matched_stars_01_epoch{0}_{1}.txt'.format(epoch, channel), format = 'csv', overwrite = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "magnitudes = [16.5776,\n",
    "              16.4341,\n",
    "              16.4177,\n",
    "              16.4532,\n",
    "              16.4204,\n",
    "              16.4732,\n",
    "              16.4265,\n",
    "              16.4563,\n",
    "              16.4105,\n",
    "              16.4760,\n",
    "              16.4761,\n",
    "              16.3585]\n",
    "\n",
    "magnitudes_err = [0.03218,\n",
    "                  0.03232,\n",
    "                  0.03095,\n",
    "                  0.03190,\n",
    "                  0.02898,\n",
    "                  0.03406,\n",
    "                  0.03141,\n",
    "                  0.03262,\n",
    "                  0.03316,\n",
    "                  0.03251,\n",
    "                  0.03097,\n",
    "                  0.03050]\n",
    "\n",
    "\n",
    "\n",
    "#    elinewidth=2,\n",
    "#    markeredgewidth=2\n",
    "    \n",
    "plt.plot(time_list, magnitudes, color = 'kx', linestyle='None', markersize = 5)\n",
    "plt.errorbar(x = time_list, y = magnitudes, xerr = None, yerr = magnitudes_err, fmt = 'o', capsize = 5)\n",
    "plt.title('RRL ID1')\n",
    "plt.xlabel('time [seconds]')\n",
    "plt.ylabel('apparent magnitude')\n",
    "plt.grid()\n",
    "plt.gca().invert_yaxis()\n",
    "plt.gcf().set_size_inches(10, 5)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
